---
title: "Prediction with Linear Modelling"
author: "Kevin Li"
date: "12/11/2020"
output: pdf_document
fontsize: 12pt
geometry: margin = 1in
---


# Summary




The objective of my analysis is to construct a fitted model based on the 1946
observations of computer-generated structures for the Covid-19 protein, and
use that fitted model to predict the values of the response variables of another
data set with a further 1946 observations. I decided to start off by 
constructing the full model with all the predictors based on the multiple 
linear regression model. I then decided to remove multicollinearity from
the 685 predictors in protein-train.csv. I assessed regression model 
assumptions, and addressed outliers, then tested different model selection 
methods on a single train/validation set.
I considered forwards selection, backwards selection, and 
forwards/backwards selection, with both Akaike information criterion (AIC) and
Bayesian information criterion (BIC). I also considered Iterative conditional
minimization (ICM) as a selection method, with AIC and BIC, and a harsher
penalty than BIC. I then chose the best candidates and cross validated them, and 
found the best model selection method, which I then applied on the full data 
set in order to get the final model. I then assessed multiple linear regression
assumptions for my final model. Finally, I used my final model to predict the 
values of the response variables of protein-test.csv. I found in my main 
results that forward selection with AIC as criteria had the lowest average root
mean squared error (RMSPE), after cross validation, and so the model
constructed with forward selection with AIC criteria would be my final model.

\newpage


# Exploratory analysis of dataset

```{r echo=FALSE}

ctrain <- read.csv ("protein-train.csv")
pfull <- lm(accuracy ~ .,data = ctrain)
par ( mfrow = c (2 ,2) )
plot(pfull$fitted.values, pfull$residuals, xlab = "Fitted Values", 
    ylab = "Residuals")

plot(1: nrow (ctrain), pfull$residuals , xlab = " Index ", ylab =
        " Residuals ")

qqnorm(pfull$residuals)
qqline(pfull$residuals, col="blue", lwd = 2)
hist(pfull$residuals)
fullrsq <- summary(pfull)$r.squared

```

In the data, there are 1946 observations and 685 predictors with 1 response.
I observed that in the data, there are no categorical predictors, and that
there are cases of perfect multicollinearity, indicating I should remove 
those predictors.
In order to take a closer look at the data, I decided to construct a full
multiple linear regression model based on all the observations and predictors 
given, and plot the residuals. In the residuals vs fitted values, we see that 
the data is randomly scattered around y=0, indicating the independence 
assumption is satisfied. We see that since the data was generated in temporal
order, we also look at the residuals vs index plot, which also satisfies model 
assumptions. 
From the histogram and QQ plot of the residuals, we see that the normality 
assumption is also satisfied. 
The R squared value of the model is `r fullrsq`, which is very large and either
indicates the full model is very accurate at predicting or overfitting. So we
have an idea on reducing predictors to get a better model for prediction.

\newpage


# Methods




In order to remove multicollinearity from the data, I used the vif function
from the "faraway" package in order to determine the variance inflation factor
(VIF) of each predictor and I then used the method of removing the predictor 
with the largest VIF in a while loop until the largest VIF was less than 10.

In order to determine whether or not I should use a model transformation, I
examined residual plots (residuals vs fitted values, residuals vs index, as
well as the QQ plot and histogram of residuals). I did not look at residuals vs
predictors since there are too many. Since the residual plots did not reveal any
problems with assumptions, I decided not to use any transformations with my
multiple linear regression model.

I also considered the effects of individual observations on my fitted multiple
linear regression model. I recorded the studentized residuals, leverages and 
Cook's distances for my model (after removing multicollinearity). I then
plotted studentized residuals vs fitted values, and noted the observations
that lay outside of 3 standard deviations. I also plotted the leverages and 
noted the leverages greater than twice the average. Then, to put it together,
I looked at a plot of the Cook's distances. Cook's distances of > 0.5 are
generally considered outliers. Thus, I found no outliers in the data.
However, I would like to note that I used 0.5 as a cutoff point since it is 
the one used in the lectures, but some other general rule of thumb cutoff
points are 4/(number of observations), or 4/(number of observations - number of 
predictors - 1). When testing the latter cutoff point, I got around 200
outliers. Due to time constraints, I decided not to test its impact on
prediction in a train/validation split, but I would have if time was not an 
issue.

Now that I removed multicollinearity and considered the effects of individual
observations, I began to consider different model selection strategies.

I noted that since there were more than 500 predictors even after removing
multicollinearity, it was too difficult to do an all possible regressions
search.
Thus I started off with a single train/validation split (80/20) and tested out 
each of forward/backward/forward-backward selections with both AIC and BIC 
as criteria.
I then also tested ICM with BIC and AIC as criteria. Additionally, I tested out
ICM with a harsher penalty of 2 * BIC's. I narrowed down to a few model
selection methods by comparing the root mean squared error for train and
validation from each model selection method. I noticed that backward selection
took much longer than forward selection or forward/backward selection and ICM 
was the fastest. AIC was also generally slower than BIC.

I cross validated the final candidates with K = 5 folds. I compared the 
average of each model's root mean square error (RMSE) to find the lowest
in order to determine the best procedure/selection method. To get my final
model, I applied this method on the full dataset.
Using this final model, I predicted the response values of protein-test.csv.

\newpage


# Results and discussion



## Results of removing multicollinearity

After removing multicollinearity, I ended up with 551 predictors, so a total of
134 predictors were removed to get rid of multicollinearity.

## Results of assessing the effects of individual observations

Using a Cook's distance cutoff point of 0.5, I observed 0 outliers
in the data.


```{r echo = FALSE}
# Cook's distance
load("premov")

cd <- cooks.distance(premov) # Cook's distance

plot(cd, ylab = "Cook's distance")
abline(h = 0.5, col = "red", lty = 2)
 h <- unname(which(cd > 0.5))
# Cook's distance greater than 0.5 is generally considered large
# Thus, there are no outliers 
# Other rules of thumb are 4/n, and 4/n-k-1

# otrain <- ptrain[-h,]
# new <- lm(accuracy ~ .,data = ptrain)
# summary(new)

```


```{r echo = FALSE}

load("ptrain")
set.seed(20688307)
N <- nrow(ptrain) 
trainInd <- sample(1: N ,round(N*0.8) ,replace = F) 
# 80/20 train/validation split
trainSet <- ptrain[trainInd ,]
validSet <- ptrain[-trainInd ,]

load("m_aicf")
load("m_bicf")
load("m_aicb")
load("m_bicb")
load("m_aich")
load("m_bich")
load("m_bestA")
load("m_bestB")
load("m_bestC")


# We calculate the RMSE on train and validation for each model


# Full model (after removing multicollinearity)

load("full")

 predfull <- predict(full, newdata = validSet) 
 fullv <- sqrt(mean((validSet$accuracy - predfull)^2)) #RMSE for validation
 fullt <- sqrt(mean(full$residuals^2)) # RMSE for train



# AIC-forward selection

 predaf <- predict(m_aicf, newdata = validSet)
 aicfv <- sqrt(mean((validSet$accuracy - predaf)^2)) 
 aicft <- sqrt(mean(m_aicf$residuals ^2)) 
 
 faic <- AIC(m_aicf)

# AIC-backward selection

 predab <- predict(m_aicb, newdata = validSet) 
 aicbv <- sqrt(mean((validSet$accuracy - predab)^2)) 
 aicbt <- sqrt(mean(m_aicb$residuals^2))
 
 baic <- AIC(m_aicb)

# AIC-forward/backward selection

 predah <- predict(m_aich, newdata = validSet)
 aichv <- sqrt(mean((validSet$accuracy - predah )^2))
 aicht <- sqrt(mean(m_aich$residuals ^2))

 haic <- AIC(m_aich)

# BIC-forward selection

 predbf <- predict (m_bicf , newdata = validSet)
 bicfv <- sqrt(mean((validSet$accuracy - predbf )^2)) 
 bicft <- sqrt(mean(m_bicf$residuals^2)) 
 
 fbic <- BIC(m_bicf)

# BIC-backward selection

 predbb <- predict(m_bicb, newdata = validSet) 
 bicbv <- sqrt(mean((validSet$accuracy - predbb)^2)) 
 bicbt <- sqrt(mean(m_bicb$residuals^2)) 
 
 bbic <- BIC(m_bicb)


# BIC-forward/backward selection

 predbh <- predict(m_bich, newdata = validSet) 
 bichv <- sqrt(mean((validSet$accuracy - predbh)^2))
 bicht <- sqrt(mean(m_bich$residuals ^2))
 
 hbic <- BIC(m_bich)


```


``` {r echo = FALSE}

# ICM RMSE - we get varying RMSE's since different orderings for ICM
# Sometimes we get a higher RMSE, sometimes lower. So we can cross validate to
# check further.

# ICM - no penalty

 bestA <- predict(bestA_model, newdata = validSet) 
 bestAv <- sqrt(mean((validSet$accuracy - bestA)^2))
 bestAt <- sqrt(mean(bestA_model$residuals ^2))
 
 bestaic <- AIC(bestA_model)


# ICM -BIC penalty

 bestB <- predict(bestB_model, newdata = validSet) 
 bestBv <- sqrt(mean((validSet$accuracy - bestB)^2))
 bestBt <- sqrt(mean(bestB_model$residuals ^2))

 bestbic <- BIC(bestB_model)
 
 # ICM -2 * BIC penalty

 bestC <- predict(bestC_model, newdata = validSet) 
 bestCv <- sqrt(mean((validSet$accuracy - bestC)^2))
 bestCt <- sqrt(mean(bestC_model$residuals ^2))

 

```


As seen in the plot, there are no points higher than y = 0.5.


## Results of model selection

During my single train/validation split, I tested a total of 9 selection 
methods. Forward/backward/forward-backward, each with both AIC and BIC criteria, 
and ICM with AIC, BIC and a 2*BIC penalty. I recorded the train RMSE and
validation RMSE for each model constructed with each of the 9 methods.


Selection method          | Train RSME     | Validation RMSE  | AIC/BIC
------------------------- | ---------------| ---------------- | ---------
Forward with AIC          | `r aicft`      | `r aicfv`        | `r faic`
Backward with AIC         | `r aicbt`      | `r aicbv`        | `r baic` 
Forward-backward with AIC | `r aicht`      | `r aichv`        | `r haic`
Forward with BIC          | `r bicft`      | `r bicfv`        | `r fbic`
Backward with BIC         | `r bicbt`      | `r bicbv`        | `r bbic`
Forward-backward BIC      | `r bicht`      | `r bichv`        | `r hbic`
ICM with AIC              | `r bestAt`     | `r bestAv`       | `r bestaic`
ICM with BIC              | `r bestBt`     | `r bestBv`       | `r bestbic`
ICM with 2*BIC penalty    | `r bestCt`     | `r bestBv`       | 
Full model                | `r fullt`      | `r fullv`



Firstly, I see that both BIC and AIC values are lowest when I used ICM,
with AIC/BIC when compared to the stepwise methods.

Since train RMSE can be made arbitrarily small by adding linearly independent
predictors, we mainly care about the lowest validation RMSE, as long as they 
are approximately equal. 

Since this is only a single train/validation split, I use this data to help me
narrow down a few candidates for K fold cross validation.

I will note here that I tested out the ICM methods multiple times, and I got
varying RMSE's for both train and validation. Sometimes they were better than
the stepwise methods and sometimes they were worse, indicating a level of
variability that I tested further with cross validation.

We can see that the validation RMSE is lower in forward with AIC(253 predictors)
than the full model, which means the full model(551 predictors) is overfitting, 
so my exploratory analysis was correct in predicting we should get rid of some
predictors. The stepwise methods with BIC as criteria yielded models with around
80 predictors, and had higher RMSE than forward with AIC, so I concluded they 
were underfitting.

With these results, I conclude that I should cross validate forward with AIC, 
forward with BIC, ICM with AIC and ICM with BIC to further evaluate selection
strategies.

I decided on cross validation with 5 folds. If I had more time, I would have
liked to use more folds.

```{r echo = FALSE}

load("RMSE1")
load("RMSE2")
load("RMSE3")
load("RMSE4")

RMSPE1 <- mean(RMSE1)
RMSPE2 <- mean(RMSE2)
RMSPE3 <- mean(RMSE3)
RMSPE4 <- mean(RMSE4)

```


The results of cross validation are

Selection method | Average RMSE (RMSPE)
---------------- | -------------
Forward AIC      | `r RMSPE1`
ICM AIC          | `r RMSPE2`
Forward BIC      | `r RMSPE3`
ICM BIC          | `r RMSPE4`

Thus I can conclude that the model constructed with forward with AIC has 
the lowest RMSPE and so I can conclude that it is the best model selection
method for prediction. I then apply this selection method to the entire data set 
to get my final model.

```{r echo=FALSE}

load("mfinal")
finalrsq <- summary(mfinal)$r.squared

```

My final model selection method evaluated 112158 models before finishing.
The R squared value for my final model is `r finalrsq`, and my final model has 
267 predictors and 1 response. A full list of these predictors can be 
found in the appendix.
The regression coefficient for the first 3 predictors are 0.033416, -0.097572   ,
and -0.052900. They are all statistically significant since the corresponding
p-values are much less than 0.005. (< 1.51e-05)

```{r echo = FALSE}

# We check the model for linear regression assumptions again


par ( mfrow = c (2 ,2) )

 plot(mfinal$fitted.values, mfinal$residuals, xlab = "Fitted Values", 
      ylab = "Residuals")
 plot (1: nrow (ptrain), mfinal$residuals , xlab = " Index ", ylab =
         " Residuals ")
 
 qqnorm(mfinal$residuals)
 qqline(mfinal$residuals, col="blue", lwd = 2)
 hist(mfinal$residuals)

MSPE <- RMSPE1^2
 
```

Overall my final model fits the data very well. All the regression assumptions 
are reasonably satisfied by my final model. In the residuals vs fitted values
and residuals vs index plot,the points are randomly scattered around y=0. In the 
QQplot and histogram, normality seems to be satisfied.

I would predict the MSPE to be `r MSPE`, if I applied the fitted model on new 
data. I am fairly confident my model should generalize well to doing prediction
on new data since my model selection strategy yielded the lowest MSPE with 
cross validation, but since I only cross validated with 5 folds, I know I'm not 
as confident as I could be. If I had more time I would try 10 or even more 
folds.

Some interesting findings are that I noticed ICM with AIC sometimes yielded
lower RMSE's than forward selection with AIC, but there was more variability,
so the average RMSE was still lower.
In addition, my final model did not include angles, a predictor that represents
a score based on the configuration of angles in the computer generated structure
of each Covid-19 spike protein.



\newpage






# Appendix




```{r}

# Detect and remove multicollinearity / commented since I saved after running

library("faraway")

ptrain <- read.csv ("protein-train.csv")

i <- 0
pred <- c()

# full model
pfull <- lm(accuracy ~ .,data = ptrain)

# vector of VIF's of predictors
# npred <- vif(pfull)
# vifpred <- unname(npred)

# copy of full model
ctrain <- ptrain[]

load("ptrain")

# # while the largest VIF >= 10
# while(max(vifpred) >= 10)
# {
#   
#   # find the index of the max VIF 
#   j <- which.max(vifpred) + 1 
#   ptrain <- ptrain[,-j] 
#   # vifpred only includes the predictors (685) while ptrain has the
#   # response as well (686), so we add 1 to j
#   
#   
#   premov <- lm(accuracy ~ .,data = ptrain) # fit new model without predictor 
#   # with largest VIF
#   
#   npred <- vif(premov)
#   vifpred <- unname(npred) # find new vector of VIF's and loop until
#   # every VIF is less than 10
#   
# 
#   i <- i + 1 # count of how many predictors removed
#   
# }
load("premov")

  
```


```{r}

# after removing multicollinearity, we assess model linear regression 
# assumptions

#summary(premov)
plot(premov$fitted.values, premov$residuals, xlab = "Fitted Values", 
     ylab = "Residuals")
plot (1: nrow (ptrain), premov$residuals , xlab = " Index ", ylab =
        " Residuals ")

qqnorm(premov$residuals)
qqline(premov$residuals, col="blue", lwd = 2)
hist(premov$residuals)

```


```{r}
library(MASS)

# quantities for individual observations
sres <- studres(premov) # studentized residuals
lev <- hatvalues(premov) # leverage
cd <- cooks.distance(premov) # Cook's distance


# plot of studentized residuals vs fitted values
plot(premov$fitted.values, sres, xlab = "Fitted values", 
     ylab = "Studentized residuals")
# plots look the same
abline(h = c(3,-3), col = "red", lty=2) # lie within 3 standard deviations
# of 0
which(abs(sres) > 3) # find the observations that don't lie within 3
# standard deviations
#  23  226  527  846 1196 1324 1344 1686 1733
qqnorm(sres)
qqline(sres, col = "blue", lwd = 2)

```


```{r}
#leverage

plot(lev, ylab = "Leverage")
abline(h = 2 * mean(lev), col = "red", lty = 2) # leverages twice than average
# leverage are considered high
which(lev > 2 * mean(lev)) # 1635


```


```{r}
# Cook's distance

plot(cd, ylab = "Cook's distance")
abline(h = 0.5, col = "red", lty = 2)
 h <- unname(which(cd > 0.5))
# Cook's distance greater than 0.5 is generally considered large
# Thus, there are no outliers 
# Other rules of thumb are 4/n, and 4/n-k-1

 otrain <- ptrain[-h,]
 new <- lm(accuracy ~ .,data = ptrain)
# summary(new)



```


```{r}

# Check model regression assumptions after outliers removed

# plot(new$fitted.values, new$residuals, xlab = "Fitted Values", 
#      ylab = "Residuals")
# qqnorm(new$residuals)
# qqline(new$residuals, col="blue", lwd = 2)
# hist(new$residuals)

```


```{r}

# First we test out different model selection methods with a single 
# train/validation split.

library(MASS)

N <- nrow(ptrain)
set.seed(20688307)

trainInd <- sample(1: N ,round(N*0.8) ,replace = F) 
# 80/20 train/validation split
trainSet <- ptrain[trainInd ,]
validSet <- ptrain[-trainInd ,]

full <- lm(accuracy ~ ., data = trainSet)
empty <- lm(accuracy ~ 1, data = trainSet)

# stepwise forward with aic

load("m_aicf")

#m_aicf <- stepAIC(object = empty, scope = list(upper = full, lower = empty), direction = #"forward")



# stepwise backward with aic

load("m_aicb")

#m_aicb <- stepAIC(object = full, scope = list(upper = full, lower = empty), direction = #"backward")



# stepwise forward/backward with aic

load("m_aich")

#m_aich <- stepAIC(object = empty, scope = list(upper = full, lower = empty), direction
#= "both")



# stepwise forward with bic

load("m_bicf")

#m_bicf <- stepAIC(object = empty, scope = list(upper = full, lower = empty), direction = #"forward", k =log(nrow(trainSet)))



# stepwise backward with bic

load("m_bicb")

#m_bicb <- stepAIC(object = full, scope = list(upper = full, lower = empty), direction = "backward", #k=log(nrow(trainSet)))



#stepwise forward-backward with bic

load("m_bich")

#m_bich <- stepAIC(object = empty, scope = list(upper = full, lower = empty), direction = "both", k #=log(nrow(trainSet)))


```


```{r}

# We calculate the RMSE on train and validation for each model


# Full model (after removing multicollinearity)

 predfull <- predict(full, newdata = validSet) 
 fullv <- sqrt(mean((validSet$accuracy - predfull)^2)) #RMSE for validation
 fullt <- sqrt(mean(full$residuals^2)) # RMSE for train



# AIC-forward selection

 predaf <- predict(m_aicf, newdata = validSet)
 aicfv <- sqrt(mean((validSet$accuracy - predaf)^2)) 
 aicft <- sqrt(mean(m_aicf$residuals ^2)) 
 
 faic <- AIC(m_aicf)

# AIC-backward selection

 predab <- predict(m_aicb, newdata = validSet) 
 aicbv <- sqrt(mean((validSet$accuracy - predab)^2)) 
 aicbt <- sqrt(mean(m_aicb$residuals^2))
 
 baic <- AIC(m_aicb)

# AIC-forward/backward selection

 predah <- predict(m_aich, newdata = validSet)
 aichv <- sqrt(mean((validSet$accuracy - predah )^2))
 aicht <- sqrt(mean(m_aich$residuals ^2))

 haic <- AIC(m_aich)

# BIC-forward selection

 predbf <- predict (m_bicf , newdata = validSet)
 bicfv <- sqrt(mean((validSet$accuracy - predbf )^2)) 
 bicft <- sqrt(mean(m_bicf$residuals^2)) 
 
 fbic <- BIC(m_bicf)

# BIC-backward selection

 predbb <- predict(m_bicb, newdata = validSet) 
 bicbv <- sqrt(mean((validSet$accuracy - predbb)^2)) 
 bicbt <- sqrt(mean(m_bicb$residuals^2)) 
 
 bbic <- BIC(m_bicb)


# BIC-forward/backward selection

 predbh <- predict(m_bich, newdata = validSet) 
 bichv <- sqrt(mean((validSet$accuracy - predbh)^2))
 bicht <- sqrt(mean(m_bich$residuals ^2))
 
 hbic <- BIC(m_bich)


```


```{r}

#ICM with no penalty (AIC)
library(MASS)

N <- nrow(ptrain)

load("m_bestA")

#set.seed(20688307)

# trainInd <- sample(1: N ,round(N*0.8) ,replace = F) 
# # 80/20 train/validation split
# trainSet <- ptrain[trainInd ,]
# validSet <- ptrain[-trainInd ,]
# 
# full <- lm(accuracy ~ ., data = trainSet)
# empty <- lm(accuracy ~ 1, data = trainSet)
# 
# varlist = c()
# varnames = names(trainSet)
# n = nrow(trainSet)
# varoder <- sample(1:ncol(trainSet)) #random order of variables
# minCrit = Inf
# noChange = F
# 
# while(!noChange) {
#   noChange = T
#   for (i in varoder) {
#     if (i == 1)
#       next
#     
#     if (i %in% varlist & length(varlist) > 1) {
#       index = c(1, varlist[varlist != i])
#     trainVars = trainSet[, index]
#     
#       fit = lm(accuracy ~ ., data = trainVars)
#       
#       if (AIC(fit) < minCrit) {
#           minCrit = AIC (fit)
#           varlist = varlist[varlist != i]
#           #print(paste0(" Criterion : ", round(minCrit, 1) , ", variables : ",
#             #           paste0 (varnames[varlist], collapse = " ")))
#           bestA_model = fit
#           noChange = F
#       }
#       
#     }  else if  (!i %in% varlist) {
#       index = c(1, varlist, i)
#       trainVars = trainSet[, index]
#     
#       fit = lm(accuracy ~ ., data = trainVars)
#     
#       if (AIC(fit) < minCrit) {
#           minCrit = AIC(fit)
#           varlist = c(varlist, i)
#          # print(paste0(" Criterion : ", round(minCrit, 1), ", variables : ",
#                    #  paste0(varnames[varlist], collapse = " ")))
#   
#           bestA_model = fit
#           noChange = F
#        }
#     } 
#   }
# }

```




```{r}

# ICM with BIC penalty
library(MASS)

N <- nrow(ptrain)
#set.seed(20688307) different AIC each time without seed

load("m_bestB")

# trainInd <- sample(1: N ,round(N*0.8) ,replace = F) 
# # 80/20 train/validation split
# trainSet <- ptrain[trainInd ,]
# validSet <- ptrain[-trainInd ,]
# 
# full <- lm(accuracy ~ ., data = trainSet)
# empty <- lm(accuracy ~ 1, data = trainSet)
# 
# pen <- log (nrow (trainSet))
# varlist = c()
# varnames = names(trainSet)
# n = nrow(trainSet)
# varoder <- sample(1:ncol(trainSet)) #random order of variables
# minCrit = Inf
# noChange = F
# 
# while(!noChange) {
#   noChange = T
#   for (i in varoder) {
#     if (i == 1)
#       next
#     
#     if (i %in% varlist & length(varlist) > 1) {
#       index = c(1, varlist[varlist != i])
#     trainVars = trainSet[, index]
#     
#       fit = lm(accuracy ~ ., data = trainVars)
#       
#       if (AIC(fit, k=pen) < minCrit) {
#           minCrit = AIC (fit, k = pen)
#           varlist = varlist[varlist!= i]
#         #  print(paste0(" Criterion : ", round(minCrit, 1) , ", variables : ",
#         #               paste0 (varnames[varlist], collapse = " ")))
#           bestB_model = fit
#           noChange = F
#       }
#       
#     }  else if  (!i %in% varlist) {
#       index = c(1, varlist, i)
#       trainVars = trainSet[, index]
#     
#       fit = lm(accuracy ~ ., data = trainVars)
#     
#       if (AIC(fit, k = pen) < minCrit) {
#           minCrit = AIC(fit, k = pen)
#           varlist = c(varlist, i)
#         #  print(paste0(" Criterion : ", round(minCrit, 1), ", variables : ",
#                 #     paste0(varnames[varlist], collapse = " ")))
#   
#           bestB_model = fit
#           noChange = F
#        }
#     } 
#   }
# }



```


```{r}

# ICM with 2*BIC penalty
library(MASS)

N <- nrow(ptrain)
#set.seed(20688307) different AIC each time without seed

load("m_bestC")

# trainInd <- sample(1: N ,round(N*0.8) ,replace = F)
# # 80/20 train/validation split
# trainSet <- ptrain[trainInd ,]
# validSet <- ptrain[-trainInd ,]
# 
# full <- lm(accuracy ~ ., data = trainSet)
# empty <- lm(accuracy ~ 1, data = trainSet)
# 
# pen <- 2 * log (nrow (trainSet))
# varlist = c()
# varnames = names(trainSet)
# n = nrow(trainSet)
# varoder <- sample(1:ncol(trainSet)) #random order of variables
# minCrit = Inf
# noChange = F
# 
# while(!noChange) {
#   noChange = T
#   for (i in varoder) {
#     if (i == 1)
#       next
# 
#     if (i %in% varlist & length(varlist) > 1) {
#       index = c(1, varlist[varlist != i])
#     trainVars = trainSet[, index]
# 
#       fit = lm(accuracy ~ ., data = trainVars)
# 
#       if (AIC(fit, k=pen) < minCrit) {
#           minCrit = AIC (fit, k = pen)
#           varlist = varlist[varlist!= i]
#         #  print(paste0(" Criterion : ", round(minCrit, 1) , ", variables : ",
#         #               paste0 (varnames[varlist], collapse = " ")))
#           bestC_model = fit
#           noChange = F
#       }
# 
#     }  else if  (!i %in% varlist) {
#       index = c(1, varlist, i)
#       trainVars = trainSet[, index]
# 
#       fit = lm(accuracy ~ ., data = trainVars)
# 
#       if (AIC(fit, k = pen) < minCrit) {
#           minCrit = AIC(fit, k = pen)
#           varlist = c(varlist, i)
#         #  print(paste0(" Criterion : ", round(minCrit, 1), ", variables : ",
#                 #     paste0(varnames[varlist], collapse = " ")))
# 
#           bestC_model = fit
#           noChange = F
#        }
#     }
#   }
# }



```


``` {r}

# ICM RMSE - we get varying RMSE's since different orderings for ICM
# Sometimes we get a higher RMSE, sometimes lower. So we can cross validate to
# check further.

# ICM - no penalty

 bestA <- predict(bestA_model, newdata = validSet) 
 bestAv <- sqrt(mean((validSet$accuracy - bestA)^2))
 bestAt <- sqrt(mean(bestA_model$residuals ^2))
 
 bestaic <- AIC(bestA_model)


# ICM -BIC penalty

 bestB <- predict(bestB_model, newdata = validSet) 
 bestBv <- sqrt(mean((validSet$accuracy - bestB)^2))
 bestBt <- sqrt(mean(bestB_model$residuals ^2))

 bestbic <- BIC(bestB_model)
 
 # ICM -2 * BIC penalty

 bestC <- predict(bestC_model, newdata = validSet) 
 bestCv <- sqrt(mean((validSet$accuracy - bestC)^2))
 bestCt <- sqrt(mean(bestC_model$residuals ^2))

 

```



```{r}

load("RMSE1")
load("RMSE2")
load("RMSE3")
load("RMSE4")

RMSPE1 <- mean(RMSE1)
RMSPE2 <- mean(RMSE2)
RMSPE3 <- mean(RMSE3)
RMSPE4 <- mean(RMSE4)


# Cross Validation to determine the best selection method
# library (MASS)
# N <- nrow(ptrain)
# set.seed(20688307)
# 
# K<-5
# 
# validSetSplits <- sample((1:N)%%K + 1)
# RMSE1 <- c()
# RMSE2 <- c()
# RMSE3 <- c()
# RMSE4 <- c()
# 
# for (p in 1:K) {
#   validSet <- ptrain[validSetSplits == p ,]
#   trainSet <- ptrain[validSetSplits != p ,]
# 
#   full <- lm (accuracy ~ . , data = trainSet)
#   empty <- lm (accuracy ~ 1 , data = trainSet)
# 
# 
# m1 <- stepAIC(object = empty, scope = list(upper = full, lower = empty), direction = "forward", trace = #0)
# 
# pred1 <- predict( m1, newdata = validSet)
# RMSE1[p] <- sqrt(mean((validSet$accuracy - pred1)^2))
# 
# 
# 
# varlist = c()
# varnames = names(trainSet)
# n = nrow(trainSet)
# varoder <- sample(1:ncol(trainSet)) #random order of variables
# minCrit = Inf
# noChange = F
# 
# while(!noChange) {
#   noChange = T
#   for (i in varoder) {
#     if (i == 1)
#       next
# 
#     if (i %in% varlist & length(varlist) > 1) {
#       index = c(1, varlist[varlist != i])
#     trainVars = trainSet[, index]
# 
#       fit = lm(accuracy ~ ., data = trainVars)
# 
#       if (AIC(fit) < minCrit) {
#           minCrit = AIC (fit)
#           varlist = varlist[varlist != i]
#           #print(paste0(" Criterion : ", round(minCrit, 1) , ", variables : ",
#                        #paste0 (varnames[varlist], collapse = " ")))
#           m2 = fit
#           noChange = F
#       }
# 
#     }  else if  (!i %in% varlist) {
#       index = c(1, varlist, i)
#       trainVars = trainSet[, index]
# 
#       fit = lm(accuracy ~ ., data = trainVars)
# 
#       if (AIC(fit) < minCrit) {
#           minCrit = AIC(fit)
#           varlist = c(varlist, i)
#          # print(paste0(" Criterion : ", round(minCrit, 1), ", variables : ",
#                   #   paste0(varnames[varlist], collapse = " ")))
# 
#            m2 = fit
#            noChange = F
#         }
#      }
#    }
# }
# 
#  pred2 <- predict(m2 ,newdata = validSet)
#  RMSE2[p] <- sqrt(mean((validSet$accuracy - pred2)^2))
# 
# 
# 
# 
#  m3 <- stepAIC(object = empty, scope = list(upper = full, lower = empty), direction = "forward", trace = 0, k =log(nrow(trainSet)))
#  pred3 <- predict(m3 , newdata = validSet )
#  RMSE3[p] <- sqrt(mean(( validSet$accuracy - pred3)^2))
# 
# 
# 
# 
# pen <- log (nrow (trainSet))
# varlist = c()
# varnames = names(trainSet)
# n = nrow(trainSet)
# varoder <- sample(1:ncol(trainSet)) #random order of variables
# minCrit = Inf
# noChange = F
# 
# while(!noChange) {
#   noChange = T
#   for (z in varoder) {
#     if (z == 1)
#       next
# 
#     if (z %in% varlist & length(varlist) > 1) {
#       index = c(1, varlist[varlist != z])
#     trainVars = trainSet[, index]
# 
#       fit = lm(accuracy ~ ., data = trainVars)
# 
#       if (AIC(fit, k=pen) < minCrit) {
#           minCrit = AIC (fit, k = pen)
#           varlist = varlist[varlist!= z]
#           #print(paste0(" Criterion : ", round(minCrit, 1) , ", variables : ",
#                        #paste0 (varnames[varlist], collapse = " ")))
#           m4 = fit
#           noChange = F
#       }
# 
#     }  else if  (!z %in% varlist) {
#       index = c(1, varlist, z)
#       trainVars = trainSet[, index]
# 
#       fit = lm(accuracy ~ ., data = trainVars)
# 
#       if (AIC(fit, k = pen) < minCrit) {
#           minCrit = AIC(fit, k = pen)
#           varlist = c(varlist, z)
#          # print(paste0(" Criterion : ", round(minCrit, 1), ", variables : ",
#                      #paste0(varnames[varlist], collapse = " ")))
# 
#           m4 = fit
#           noChange = F
#        }
#     }
#   }
# }
# 
# pred4 <- predict(m4, newdata = validSet)
# RMSE4[p] <- sqrt(mean((validSet$accuracy - pred4)^2))
# 
#                   
# }              
#                   
#                   

```




```{r}

# We apply selected procedure to the full dataset

load("mfinal")

# full <- lm(accuracy ~ ., data = ptrain)
# empty <- lm(accuracy ~ 1, data = ptrain)
# 
# mfinal <- stepAIC(object = empty, scope = list(upper = full, lower = empty), direction = "forward", trace = 0)

mfinal$coefficients

```




```{r}

# We check the model for linear regression assumptions again


 #summary(premov)
par ( mfrow = c (2 ,2) )

 plot(mfinal$fitted.values, mfinal$residuals, xlab = "Fitted Values", 
      ylab = "Residuals")
 plot (1: nrow (ptrain), mfinal$residuals , xlab = " Index ", ylab =
         " Residuals ")
 
 qqnorm(mfinal$residuals)
 qqline(mfinal$residuals, col="blue", lwd = 2)
 hist(mfinal$residuals)


```



```{r}
# use final model to predict response for protein-test.csv

ptest <- read.csv ("protein-test.csv")


predfinal <- predict(mfinal, newdata = ptest)

writeLines(as.character(predfinal), "mypreds1.txt.txt")




```









